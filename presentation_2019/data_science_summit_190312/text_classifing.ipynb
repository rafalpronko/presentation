{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Klasyfikacja krotkich tekstow na przykladzie nazw zawodow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## O Mnie\n",
    "\n",
    "**Rafal Pronko**\n",
    "\n",
    "Jako DS pracowalem / Pracuje:\n",
    "- Centrum Zastosowan Matematyki i Inzynierii Systemow\n",
    "- Webinterpret (NLP / Churn prediction model)\n",
    "- YND (NLP / Image processing) / Private Matter - blockchain scientist\n",
    "- CVTimeline (NLP)\n",
    "\n",
    "Mozna mnie spotkac:\n",
    "- meetupy DS / Machinelearning\n",
    "- Kariera IT - prelegent\n",
    "- WDI - prelegent\n",
    "- IT career summit - prelegent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP\n",
    "\n",
    "interdyscyplinarna dziedzina, łącząca zagadnienia sztucznej inteligencji i językoznawstwa, zajmująca się automatyzacją analizy, rozumienia, tłumaczenia i generowania języka naturalnego przez komputer (wikipedia)\n",
    "\n",
    "Główne zagadnienia w obrębie NLP:\n",
    "- tworzenie krótkich opisów z długich tekstów\n",
    "- analiza sentymentu\n",
    "- **klasyfikacja tekstu**\n",
    "- ekstrakcja informacji z tekstu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Klasyfikacja tekstu\n",
    "\n",
    "przypisanie predefiniowanych kategorii dla tekstu pisanego w języku naturalnym. \n",
    "\n",
    "![title](img1.png)\n",
    "https://developers.google.com/machine-learning/guides/text-classification/\n",
    "\n",
    "\n",
    "## Klasyfikacja tekstu ma wiele zastosowań:\n",
    "- Kategoryzacja ogłoszeń na portalach (Ebay / Amazon / Allegro ...)\n",
    "- Wykrywanie niechcianych tekstów: SPAM / mowa nienawiści ...\n",
    "- Klasyfikacja artykułów: przypisywanie kategorii / wykrywanie nieprawdziwych informacji\n",
    "- Klasyfikacja zawodów ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Klasyfikacja nazw zawodów\n",
    "\n",
    "Dlaczego:\n",
    "- HR\n",
    "- Standaryzacja nazw - roznego typu statystyki\n",
    "- Przewidywanie zachowan pracownikow\n",
    "- Przewidywanie dlugosci zatrudnienia\n",
    "- Wyszukiwanie odpowiedniej pracy dla danej osoby\n",
    "- Przypisac niezbedne umiejetnosci\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CVTimeline\n",
    "\n",
    "![img](img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![budowa modelu](img2.png)\n",
    "\n",
    "https://developers.google.com/machine-learning/guides/text-classification/\n",
    "\n",
    "Dane pochodza z serwisu https://www.onetcenter.org/dictionary/23.1/excel/alternate_titles.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"data.csv\", delimiter=\";\")\n",
    "df = df.reset_index().set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Alternate Title</th>\n",
       "      <th>Short Title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Aeronautics Commission Director</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Agricultural Services Director</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Alcohol and Drug Abuse Assistance Program Admi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Arts and Humanities Council Director</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Bakery Manager</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Title                                    Alternate Title  \\\n",
       "index                                                                        \n",
       "0      Chief Executives                    Aeronautics Commission Director   \n",
       "1      Chief Executives                     Agricultural Services Director   \n",
       "2      Chief Executives  Alcohol and Drug Abuse Assistance Program Admi...   \n",
       "3      Chief Executives               Arts and Humanities Council Director   \n",
       "4      Chief Executives                                     Bakery Manager   \n",
       "\n",
       "      Short Title  \n",
       "index              \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59583"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "res = df.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.2s\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    unique_titles = res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       Chief Executives\n",
       "1          Chief Sustainability Officers\n",
       "2        General and Operations Managers\n",
       "3                            Legislators\n",
       "4    Advertising and Promotions Managers\n",
       "Name: Title, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.2s\n"
     ]
    }
   ],
   "source": [
    "res = df.Title.str.lower()\n",
    "with ProgressBar():\n",
    "    df = df.assign(title_lower=res.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.3s\n"
     ]
    }
   ],
   "source": [
    "res = df[\"Alternate Title\"].str.lower()\n",
    "with ProgressBar():\n",
    "    df = df.assign(al_title_lower=res.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Alternate Title</th>\n",
       "      <th>Short Title</th>\n",
       "      <th>title_lower</th>\n",
       "      <th>al_title_lower</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Aeronautics Commission Director</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chief executives</td>\n",
       "      <td>aeronautics commission director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Agricultural Services Director</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chief executives</td>\n",
       "      <td>agricultural services director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Alcohol and Drug Abuse Assistance Program Admi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chief executives</td>\n",
       "      <td>alcohol and drug abuse assistance program admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Arts and Humanities Council Director</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chief executives</td>\n",
       "      <td>arts and humanities council director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chief Executives</td>\n",
       "      <td>Bakery Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chief executives</td>\n",
       "      <td>bakery manager</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Title                                    Alternate Title  \\\n",
       "index                                                                        \n",
       "0      Chief Executives                    Aeronautics Commission Director   \n",
       "1      Chief Executives                     Agricultural Services Director   \n",
       "2      Chief Executives  Alcohol and Drug Abuse Assistance Program Admi...   \n",
       "3      Chief Executives               Arts and Humanities Council Director   \n",
       "4      Chief Executives                                     Bakery Manager   \n",
       "\n",
       "      Short Title       title_lower  \\\n",
       "index                                 \n",
       "0             NaN  chief executives   \n",
       "1             NaN  chief executives   \n",
       "2             NaN  chief executives   \n",
       "3             NaN  chief executives   \n",
       "4             NaN  chief executives   \n",
       "\n",
       "                                          al_title_lower  \n",
       "index                                                     \n",
       "0                        aeronautics commission director  \n",
       "1                         agricultural services director  \n",
       "2      alcohol and drug abuse assistance program admi...  \n",
       "3                   arts and humanities council director  \n",
       "4                                         bakery manager  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"al_title_lower\"].compute(), df[\"title_lower\"].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44687,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reprezentacja tekstu przy klasyfikacji:\n",
    "- one hot encoder - zadziala wtedy gdy bedziemy probowac klasyfikowac te same teksty caly czas - nie bedzie generalizowac\n",
    "- bag of word - bedzie generalizowac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([(\"text_processing\", CountVectorizer()), (\"clf\", MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"text_processing\": [CountVectorizer(), TfidfVectorizer()],\n",
    "    \"text_processing__ngram_range\": [(1,1), (1,2), (1,3)],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf, param_grid=param, verbose=5, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1), score=0.1696466581524053, total=   3.1s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1), score=0.16710353866317168, total=   4.1s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    9.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1), score=0.16920492721164615, total=   3.3s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   13.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1), score=0.17488532110091742, total=   3.1s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   17.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 1), score=0.18114319387153802, total=   3.3s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2), score=0.1826309067688378, total=   6.2s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2), score=0.18020969855832242, total=   5.6s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2), score=0.17849944008958565, total=   8.9s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2), score=0.19002293577981652, total=   8.8s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 2), score=0.19292869770182675, total=   5.7s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3), score=0.18071519795657726, total=   7.7s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3), score=0.1807557885539537, total=   7.6s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3), score=0.17782754759238523, total=  11.0s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3), score=0.19036697247706422, total=   9.6s\n",
      "[CV] text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  text_processing=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), text_processing__ngram_range=(1, 3), score=0.19198585739540366, total=  11.4s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1), score=0.0772669220945083, total=   3.5s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1), score=0.07983835736129315, total=   3.0s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1), score=0.0832026875699888, total=   3.5s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1), score=0.08176605504587156, total=   3.6s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 1), score=0.08650559811431939, total=   4.0s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2), score=0.07120051085568327, total=   6.3s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2), score=0.07208387942332896, total=   7.5s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2), score=0.07469204927211646, total=   6.7s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2), score=0.07431192660550459, total=   6.9s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 2), score=0.07919858573954036, total=   8.0s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3), score=0.06992337164750957, total=   7.8s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3), score=0.0709916994320664, total=   8.7s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3), score=0.07301231802911534, total=   9.6s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3), score=0.07327981651376146, total=   8.9s\n",
      "[CV] text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3) \n",
      "[CV]  text_processing=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), text_processing__ngram_range=(1, 3), score=0.07719505008839128, total=  11.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  4.2min finished\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('text_processing', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), p...nizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'text_processing': [CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=Non..., use_idf=True,\n",
       "        vocabulary=None)], 'text_processing__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18470696175621545"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text_processing', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1871643394199785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred_baseline = grid.predict(X_test)\n",
    "print(accuracy_score(y_test, pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Google radzi: \n",
    "\n",
    "1. Oblicz #liczba_przykladow / #liczba_slow_per_przyklad\n",
    "2. Jezeli wynik jest mniejszy niz 1500 tokenizuj tekst uzywajac n-gramow i uzyj MLP do klasyfikacji\n",
    "    - uzyj tokenizacji na poziomie slow (znakow)\n",
    "    - wybierz okolo 20K najbardziej istotnych ngramow\n",
    "    - zbuduj model\n",
    "3. Jezeli wynik jest wiekszy niz 1500 tokenizuj tekst jako sekwencje i nastepnie uzyj sepCNN\n",
    "    - podziel sentencje na slowa i wybierz 20K najlepszych slow ze wzgledu na frekwencje\n",
    "    - zmien reprezentacje na reprezentacje sekwencyjna\n",
    "    - jesli wynik z punktu 1 jest mniejszy niz 15K to uzyj pretrenowanego embedingu i spuCNN\n",
    "4. Optymalizuj model aby dobrac najlepsze parametry\n",
    "   \n",
    "https://developers.google.com/machine-learning/guides/text-classification/step-2-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "number_words = [len(x.split(\" \")) for x in X_train.values]\n",
    "avg_words_per_sample = sum(number_words) / X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17343.025359770025"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] / avg_words_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# kod pochodzi ze strony https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequence_vectorize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-81f053dbdfff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence_vectorize' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test, idx, max_length = sequence_vectorize(X_train.values, X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_labs = label_encoder.fit_transform(y_train)\n",
    "y_test_labs = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# kod pochodzi ze strony https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# kod pochodzi ze strony https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = sepcnn_model(blocks=2,\n",
    "                     filters=32,\n",
    "                     kernel_size=5,\n",
    "                     embedding_dim=300,\n",
    "                     dropout_rate=0.3,\n",
    "                     pool_size=3,\n",
    "                     input_shape=(max_length,),\n",
    "                     num_classes=1109,\n",
    "                     num_features=TOP_K,\n",
    "                     use_pretrained_embedding=False,\n",
    "                     is_embedding_trainable=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 24, 300)           6000000   \n",
      "_________________________________________________________________\n",
      "dropout_59 (Dropout)         (None, 24, 300)           0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_117 (Separa (None, 24, 32)            11132     \n",
      "_________________________________________________________________\n",
      "separable_conv1d_118 (Separa (None, 24, 32)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_119 (Separa (None, 8, 64)             2272      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_120 (Separa (None, 8, 64)             4480      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_16  (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1109)              72085     \n",
      "=================================================================\n",
      "Total params: 6,091,185\n",
      "Trainable params: 6,091,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000902  , 0.00089847, 0.00089884, ..., 0.00090491, 0.00090141,\n",
       "        0.00090136],\n",
       "       [0.000902  , 0.00089847, 0.00089884, ..., 0.00090491, 0.00090141,\n",
       "        0.00090136]], dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-3), loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44687 samples, validate on 14896 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44687/44687 [==============================] - 15s 325us/sample - loss: 6.8657 - accuracy: 0.0262 - val_loss: 6.4795 - val_accuracy: 0.0435\n",
      "Epoch 2/100\n",
      "44687/44687 [==============================] - 15s 339us/sample - loss: 6.4739 - accuracy: 0.0470 - val_loss: 6.4447 - val_accuracy: 0.0435\n",
      "Epoch 3/100\n",
      "44687/44687 [==============================] - 15s 330us/sample - loss: 6.3891 - accuracy: 0.0470 - val_loss: 6.2471 - val_accuracy: 0.0435\n",
      "Epoch 4/100\n",
      "44687/44687 [==============================] - 15s 330us/sample - loss: 6.1102 - accuracy: 0.0485 - val_loss: 6.0005 - val_accuracy: 0.0473\n",
      "Epoch 5/100\n",
      "44687/44687 [==============================] - 15s 332us/sample - loss: 5.9344 - accuracy: 0.0497 - val_loss: 5.9018 - val_accuracy: 0.0471\n",
      "Epoch 6/100\n",
      "44687/44687 [==============================] - 15s 325us/sample - loss: 5.8352 - accuracy: 0.0508 - val_loss: 5.8365 - val_accuracy: 0.0472\n",
      "Epoch 7/100\n",
      "44687/44687 [==============================] - 14s 322us/sample - loss: 5.7458 - accuracy: 0.0516 - val_loss: 5.7540 - val_accuracy: 0.0490\n",
      "Epoch 8/100\n",
      "44687/44687 [==============================] - 15s 335us/sample - loss: 5.6638 - accuracy: 0.0550 - val_loss: 5.6997 - val_accuracy: 0.0542\n",
      "Epoch 9/100\n",
      "44687/44687 [==============================] - 15s 333us/sample - loss: 5.5951 - accuracy: 0.0595 - val_loss: 5.6386 - val_accuracy: 0.0563\n",
      "Epoch 10/100\n",
      "44687/44687 [==============================] - 15s 334us/sample - loss: 5.5288 - accuracy: 0.0624 - val_loss: 5.6287 - val_accuracy: 0.0591\n",
      "Epoch 11/100\n",
      "44687/44687 [==============================] - 15s 325us/sample - loss: 5.4704 - accuracy: 0.0654 - val_loss: 5.5695 - val_accuracy: 0.0649\n",
      "Epoch 12/100\n",
      "44687/44687 [==============================] - 15s 325us/sample - loss: 5.3945 - accuracy: 0.0696 - val_loss: 5.5284 - val_accuracy: 0.0676\n",
      "Epoch 13/100\n",
      "44687/44687 [==============================] - 15s 328us/sample - loss: 5.3310 - accuracy: 0.0729 - val_loss: 5.4978 - val_accuracy: 0.0714\n",
      "Epoch 14/100\n",
      "44687/44687 [==============================] - 15s 327us/sample - loss: 5.2764 - accuracy: 0.0752 - val_loss: 5.4572 - val_accuracy: 0.0723\n",
      "Epoch 15/100\n",
      "44687/44687 [==============================] - 15s 339us/sample - loss: 5.2187 - accuracy: 0.0792 - val_loss: 5.4319 - val_accuracy: 0.0773\n",
      "Epoch 16/100\n",
      "44687/44687 [==============================] - 15s 343us/sample - loss: 5.1564 - accuracy: 0.0798 - val_loss: 5.3987 - val_accuracy: 0.0775\n",
      "Epoch 17/100\n",
      "44687/44687 [==============================] - 14s 321us/sample - loss: 5.0850 - accuracy: 0.0843 - val_loss: 5.3564 - val_accuracy: 0.0788\n",
      "Epoch 18/100\n",
      "44687/44687 [==============================] - 15s 328us/sample - loss: 5.0156 - accuracy: 0.0863 - val_loss: 5.3268 - val_accuracy: 0.0796\n",
      "Epoch 19/100\n",
      "44687/44687 [==============================] - 15s 336us/sample - loss: 4.9484 - accuracy: 0.0896 - val_loss: 5.2974 - val_accuracy: 0.0810\n",
      "Epoch 20/100\n",
      "44687/44687 [==============================] - 16s 367us/sample - loss: 4.8840 - accuracy: 0.0926 - val_loss: 5.2496 - val_accuracy: 0.0830\n",
      "Epoch 21/100\n",
      "44687/44687 [==============================] - 17s 373us/sample - loss: 4.8107 - accuracy: 0.0966 - val_loss: 5.2574 - val_accuracy: 0.0845\n",
      "Epoch 22/100\n",
      "44687/44687 [==============================] - 17s 383us/sample - loss: 4.7413 - accuracy: 0.1014 - val_loss: 5.2028 - val_accuracy: 0.0872\n",
      "Epoch 23/100\n",
      "44687/44687 [==============================] - 16s 366us/sample - loss: 4.6871 - accuracy: 0.1016 - val_loss: 5.1884 - val_accuracy: 0.0885\n",
      "Epoch 24/100\n",
      "44687/44687 [==============================] - 17s 372us/sample - loss: 4.6280 - accuracy: 0.1063 - val_loss: 5.1739 - val_accuracy: 0.0913\n",
      "Epoch 25/100\n",
      "44687/44687 [==============================] - 16s 367us/sample - loss: 4.5696 - accuracy: 0.1081 - val_loss: 5.1556 - val_accuracy: 0.0926\n",
      "Epoch 26/100\n",
      "44687/44687 [==============================] - 16s 358us/sample - loss: 4.5178 - accuracy: 0.1137 - val_loss: 5.1507 - val_accuracy: 0.0926\n",
      "Epoch 27/100\n",
      "44687/44687 [==============================] - 15s 334us/sample - loss: 4.4727 - accuracy: 0.1141 - val_loss: 5.0975 - val_accuracy: 0.0969\n",
      "Epoch 28/100\n",
      "44687/44687 [==============================] - 16s 358us/sample - loss: 4.4203 - accuracy: 0.1192 - val_loss: 5.1018 - val_accuracy: 0.0983\n",
      "Epoch 29/100\n",
      "44687/44687 [==============================] - 15s 339us/sample - loss: 4.3677 - accuracy: 0.1220 - val_loss: 5.1207 - val_accuracy: 0.1022\n",
      "Epoch 30/100\n",
      "44687/44687 [==============================] - 15s 331us/sample - loss: 4.3222 - accuracy: 0.1263 - val_loss: 5.0567 - val_accuracy: 0.1069\n",
      "Epoch 31/100\n",
      "44687/44687 [==============================] - 14s 323us/sample - loss: 4.2688 - accuracy: 0.1295 - val_loss: 5.0830 - val_accuracy: 0.1041\n",
      "Epoch 32/100\n",
      "44687/44687 [==============================] - 15s 333us/sample - loss: 4.2173 - accuracy: 0.1355 - val_loss: 5.0666 - val_accuracy: 0.1065\n",
      "Epoch 33/100\n",
      "44687/44687 [==============================] - 15s 345us/sample - loss: 4.1693 - accuracy: 0.1384 - val_loss: 5.0367 - val_accuracy: 0.1111\n",
      "Epoch 34/100\n",
      "44687/44687 [==============================] - 15s 333us/sample - loss: 4.1166 - accuracy: 0.1434 - val_loss: 5.0688 - val_accuracy: 0.1124\n",
      "Epoch 35/100\n",
      "44687/44687 [==============================] - 16s 368us/sample - loss: 4.0647 - accuracy: 0.1492 - val_loss: 4.9930 - val_accuracy: 0.1199\n",
      "Epoch 36/100\n",
      "44687/44687 [==============================] - 14s 317us/sample - loss: 4.0137 - accuracy: 0.1549 - val_loss: 5.0177 - val_accuracy: 0.1227\n",
      "Epoch 37/100\n",
      "44687/44687 [==============================] - 16s 355us/sample - loss: 3.9632 - accuracy: 0.1625 - val_loss: 5.0118 - val_accuracy: 0.1245\n",
      "Epoch 38/100\n",
      "44687/44687 [==============================] - 17s 370us/sample - loss: 3.9132 - accuracy: 0.1688 - val_loss: 5.0143 - val_accuracy: 0.1290\n",
      "Epoch 39/100\n",
      "44687/44687 [==============================] - 17s 372us/sample - loss: 3.8694 - accuracy: 0.1726 - val_loss: 5.0122 - val_accuracy: 0.1261\n",
      "Epoch 40/100\n",
      "44687/44687 [==============================] - 16s 358us/sample - loss: 3.8219 - accuracy: 0.1780 - val_loss: 5.0300 - val_accuracy: 0.1315\n",
      "Epoch 41/100\n",
      "44687/44687 [==============================] - 16s 364us/sample - loss: 3.7827 - accuracy: 0.1834 - val_loss: 5.0007 - val_accuracy: 0.1307\n",
      "Epoch 42/100\n",
      "44687/44687 [==============================] - 18s 393us/sample - loss: 3.7448 - accuracy: 0.1881 - val_loss: 4.9889 - val_accuracy: 0.1365\n",
      "Epoch 43/100\n",
      "44687/44687 [==============================] - 17s 375us/sample - loss: 3.7000 - accuracy: 0.1938 - val_loss: 5.0629 - val_accuracy: 0.1345\n",
      "Epoch 44/100\n",
      "44687/44687 [==============================] - 15s 330us/sample - loss: 3.6661 - accuracy: 0.1984 - val_loss: 5.0382 - val_accuracy: 0.1404\n",
      "Epoch 45/100\n",
      "44687/44687 [==============================] - 15s 328us/sample - loss: 3.6416 - accuracy: 0.1997 - val_loss: 5.0690 - val_accuracy: 0.1391\n",
      "Epoch 46/100\n",
      "44687/44687 [==============================] - 16s 366us/sample - loss: 3.5971 - accuracy: 0.2087 - val_loss: 5.0511 - val_accuracy: 0.1435\n",
      "Epoch 47/100\n",
      "44687/44687 [==============================] - 15s 346us/sample - loss: 3.5680 - accuracy: 0.2115 - val_loss: 5.0812 - val_accuracy: 0.1453\n",
      "Epoch 48/100\n",
      "44687/44687 [==============================] - 14s 320us/sample - loss: 3.5280 - accuracy: 0.2210 - val_loss: 5.0737 - val_accuracy: 0.1505\n",
      "Epoch 49/100\n",
      "44687/44687 [==============================] - 16s 350us/sample - loss: 3.4943 - accuracy: 0.2223 - val_loss: 5.1235 - val_accuracy: 0.1499\n",
      "Epoch 50/100\n",
      "44687/44687 [==============================] - 15s 346us/sample - loss: 3.4660 - accuracy: 0.2266 - val_loss: 5.0656 - val_accuracy: 0.1500\n",
      "Epoch 51/100\n",
      "44687/44687 [==============================] - 16s 359us/sample - loss: 3.4316 - accuracy: 0.2305 - val_loss: 5.1512 - val_accuracy: 0.1512\n",
      "Epoch 52/100\n",
      "44687/44687 [==============================] - 15s 329us/sample - loss: 3.4152 - accuracy: 0.2359 - val_loss: 5.0836 - val_accuracy: 0.1553\n",
      "Epoch 53/100\n",
      "44687/44687 [==============================] - 14s 319us/sample - loss: 3.3833 - accuracy: 0.2380 - val_loss: 5.0960 - val_accuracy: 0.1578\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44687/44687 [==============================] - 15s 346us/sample - loss: 3.3517 - accuracy: 0.2435 - val_loss: 5.1236 - val_accuracy: 0.1581\n",
      "Epoch 55/100\n",
      "44687/44687 [==============================] - 16s 354us/sample - loss: 3.3341 - accuracy: 0.2470 - val_loss: 5.1676 - val_accuracy: 0.1584\n",
      "Epoch 56/100\n",
      "44687/44687 [==============================] - 14s 306us/sample - loss: 3.2974 - accuracy: 0.2521 - val_loss: 5.1325 - val_accuracy: 0.1615\n",
      "Epoch 57/100\n",
      "44687/44687 [==============================] - 16s 366us/sample - loss: 3.2726 - accuracy: 0.2571 - val_loss: 5.1827 - val_accuracy: 0.1606\n",
      "Epoch 58/100\n",
      "44687/44687 [==============================] - 22s 487us/sample - loss: 3.2533 - accuracy: 0.2590 - val_loss: 5.1978 - val_accuracy: 0.1663\n",
      "Epoch 59/100\n",
      "44687/44687 [==============================] - 17s 384us/sample - loss: 3.2165 - accuracy: 0.2623 - val_loss: 5.2074 - val_accuracy: 0.1668\n",
      "Epoch 60/100\n",
      "44687/44687 [==============================] - 14s 323us/sample - loss: 3.1922 - accuracy: 0.2678 - val_loss: 5.1909 - val_accuracy: 0.1686\n",
      "Epoch 61/100\n",
      "44687/44687 [==============================] - 16s 359us/sample - loss: 3.1751 - accuracy: 0.2696 - val_loss: 5.3434 - val_accuracy: 0.1678\n",
      "Epoch 62/100\n",
      "44687/44687 [==============================] - 15s 344us/sample - loss: 3.1456 - accuracy: 0.2754 - val_loss: 5.2788 - val_accuracy: 0.1713\n",
      "Epoch 63/100\n",
      "44687/44687 [==============================] - 20s 440us/sample - loss: 3.1210 - accuracy: 0.2775 - val_loss: 5.3526 - val_accuracy: 0.1738\n",
      "Epoch 64/100\n",
      "44687/44687 [==============================] - 19s 416us/sample - loss: 3.0940 - accuracy: 0.2832 - val_loss: 5.2080 - val_accuracy: 0.1724\n",
      "Epoch 65/100\n",
      "44687/44687 [==============================] - 17s 370us/sample - loss: 3.0715 - accuracy: 0.2871 - val_loss: 5.2562 - val_accuracy: 0.1760\n",
      "Epoch 66/100\n",
      "44687/44687 [==============================] - 18s 394us/sample - loss: 3.0516 - accuracy: 0.2897 - val_loss: 5.2961 - val_accuracy: 0.1768\n",
      "Epoch 67/100\n",
      "44687/44687 [==============================] - 15s 336us/sample - loss: 3.0270 - accuracy: 0.2942 - val_loss: 5.2602 - val_accuracy: 0.1806\n",
      "Epoch 68/100\n",
      "44687/44687 [==============================] - 14s 324us/sample - loss: 3.0023 - accuracy: 0.2972 - val_loss: 5.3566 - val_accuracy: 0.1800\n",
      "Epoch 69/100\n",
      "44687/44687 [==============================] - 19s 436us/sample - loss: 2.9821 - accuracy: 0.3007 - val_loss: 5.3957 - val_accuracy: 0.1829\n",
      "Epoch 70/100\n",
      "44687/44687 [==============================] - 17s 380us/sample - loss: 2.9674 - accuracy: 0.3020 - val_loss: 5.3195 - val_accuracy: 0.1819\n",
      "Epoch 71/100\n",
      "44687/44687 [==============================] - 16s 350us/sample - loss: 2.9460 - accuracy: 0.3058 - val_loss: 5.4169 - val_accuracy: 0.1844\n",
      "Epoch 72/100\n",
      "44687/44687 [==============================] - 16s 349us/sample - loss: 2.9267 - accuracy: 0.3117 - val_loss: 5.4008 - val_accuracy: 0.1839\n",
      "Epoch 73/100\n",
      "44687/44687 [==============================] - 16s 352us/sample - loss: 2.8961 - accuracy: 0.3154 - val_loss: 5.4096 - val_accuracy: 0.1863\n",
      "Epoch 74/100\n",
      "44687/44687 [==============================] - 15s 338us/sample - loss: 2.8843 - accuracy: 0.3184 - val_loss: 5.4650 - val_accuracy: 0.1896\n",
      "Epoch 75/100\n",
      "44687/44687 [==============================] - 15s 342us/sample - loss: 2.8570 - accuracy: 0.3235 - val_loss: 5.4454 - val_accuracy: 0.1863\n",
      "Epoch 76/100\n",
      "44687/44687 [==============================] - 15s 332us/sample - loss: 2.8395 - accuracy: 0.3239 - val_loss: 5.5755 - val_accuracy: 0.1882\n",
      "Epoch 77/100\n",
      "44687/44687 [==============================] - 15s 343us/sample - loss: 2.8237 - accuracy: 0.3270 - val_loss: 5.4643 - val_accuracy: 0.1923\n",
      "Epoch 78/100\n",
      "44687/44687 [==============================] - 16s 368us/sample - loss: 2.8010 - accuracy: 0.3284 - val_loss: 5.6125 - val_accuracy: 0.1929\n",
      "Epoch 79/100\n",
      "44687/44687 [==============================] - 17s 370us/sample - loss: 2.7768 - accuracy: 0.3352 - val_loss: 5.5820 - val_accuracy: 0.1939\n",
      "Epoch 80/100\n",
      "44687/44687 [==============================] - 16s 352us/sample - loss: 2.7631 - accuracy: 0.3415 - val_loss: 5.5461 - val_accuracy: 0.1969\n",
      "Epoch 81/100\n",
      "44687/44687 [==============================] - 14s 310us/sample - loss: 2.7487 - accuracy: 0.3419 - val_loss: 5.6673 - val_accuracy: 0.1963\n",
      "Epoch 82/100\n",
      "44687/44687 [==============================] - 16s 360us/sample - loss: 2.7292 - accuracy: 0.3443 - val_loss: 5.5881 - val_accuracy: 0.1967\n",
      "Epoch 83/100\n",
      "44687/44687 [==============================] - 16s 360us/sample - loss: 2.6989 - accuracy: 0.3484 - val_loss: 5.6561 - val_accuracy: 0.2001\n",
      "Epoch 84/100\n",
      "44687/44687 [==============================] - 14s 321us/sample - loss: 2.6912 - accuracy: 0.3516 - val_loss: 5.5403 - val_accuracy: 0.1994\n",
      "Epoch 85/100\n",
      "44687/44687 [==============================] - 16s 354us/sample - loss: 2.6878 - accuracy: 0.3503 - val_loss: 5.6635 - val_accuracy: 0.1998\n",
      "Epoch 86/100\n",
      "44687/44687 [==============================] - 15s 343us/sample - loss: 2.6689 - accuracy: 0.3521 - val_loss: 5.7585 - val_accuracy: 0.2003\n",
      "Epoch 87/100\n",
      "44687/44687 [==============================] - 15s 340us/sample - loss: 2.6546 - accuracy: 0.3580 - val_loss: 5.8369 - val_accuracy: 0.2018\n",
      "Epoch 88/100\n",
      "44687/44687 [==============================] - 16s 347us/sample - loss: 2.6218 - accuracy: 0.3655 - val_loss: 5.7295 - val_accuracy: 0.2025\n",
      "Epoch 89/100\n",
      "44687/44687 [==============================] - 15s 342us/sample - loss: 2.6133 - accuracy: 0.3662 - val_loss: 5.6642 - val_accuracy: 0.2021\n",
      "Epoch 90/100\n",
      "44687/44687 [==============================] - 15s 342us/sample - loss: 2.5997 - accuracy: 0.3670 - val_loss: 5.7589 - val_accuracy: 0.2066\n",
      "Epoch 91/100\n",
      "44687/44687 [==============================] - 16s 351us/sample - loss: 2.5918 - accuracy: 0.3681 - val_loss: 5.8188 - val_accuracy: 0.2082\n",
      "Epoch 92/100\n",
      "44687/44687 [==============================] - 15s 335us/sample - loss: 2.5751 - accuracy: 0.3693 - val_loss: 5.8727 - val_accuracy: 0.2082\n",
      "Epoch 93/100\n",
      "44687/44687 [==============================] - 16s 359us/sample - loss: 2.5670 - accuracy: 0.3747 - val_loss: 5.8823 - val_accuracy: 0.2088\n",
      "Epoch 94/100\n",
      "44687/44687 [==============================] - 15s 329us/sample - loss: 2.5493 - accuracy: 0.3740 - val_loss: 5.9088 - val_accuracy: 0.2071\n",
      "Epoch 95/100\n",
      "44687/44687 [==============================] - 16s 352us/sample - loss: 2.5382 - accuracy: 0.3761 - val_loss: 5.8477 - val_accuracy: 0.2104\n",
      "Epoch 96/100\n",
      "44687/44687 [==============================] - 15s 345us/sample - loss: 2.5152 - accuracy: 0.3825 - val_loss: 6.0097 - val_accuracy: 0.2101\n",
      "Epoch 97/100\n",
      "44687/44687 [==============================] - 15s 332us/sample - loss: 2.5142 - accuracy: 0.3808 - val_loss: 5.9562 - val_accuracy: 0.2099\n",
      "Epoch 98/100\n",
      "44687/44687 [==============================] - 15s 333us/sample - loss: 2.4927 - accuracy: 0.3854 - val_loss: 5.8587 - val_accuracy: 0.2095\n",
      "Epoch 99/100\n",
      "44687/44687 [==============================] - 16s 351us/sample - loss: 2.4875 - accuracy: 0.3858 - val_loss: 6.0491 - val_accuracy: 0.2129\n",
      "Epoch 100/100\n",
      "44687/44687 [==============================] - 16s 354us/sample - loss: 2.4676 - accuracy: 0.3921 - val_loss: 6.0911 - val_accuracy: 0.2136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3546c44a8>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,\n",
    "          y_train_labs,\n",
    "          epochs=100,\n",
    "          validation_data=(x_test, y_test_labs),\n",
    "          batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "from tensorflow.python.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def gru_model(num_features, embedding_dim, input_shape, num_classes):\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(Bidirectional(GRU(64, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model_gru = gru_model(num_features=TOP_K,embedding_dim=250,input_shape=(max_length,),num_classes=1109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 24, 250)           5000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 24, 256)           291072    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               123264    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1109)              143061    \n",
      "=================================================================\n",
      "Total params: 5,557,397\n",
      "Trainable params: 5,557,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.compile(optimizer=Adam(lr=1e-3), loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44687 samples, validate on 14896 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44687/44687 [==============================] - 73s 2ms/sample - loss: 6.5642 - accuracy: 0.0457 - val_loss: 6.4367 - val_accuracy: 0.0435\n",
      "Epoch 2/100\n",
      "44687/44687 [==============================] - 69s 2ms/sample - loss: 6.1532 - accuracy: 0.0617 - val_loss: 5.8778 - val_accuracy: 0.0673\n",
      "Epoch 3/100\n",
      "44687/44687 [==============================] - 66s 1ms/sample - loss: 5.6369 - accuracy: 0.0797 - val_loss: 5.4942 - val_accuracy: 0.0863\n",
      "Epoch 4/100\n",
      "44687/44687 [==============================] - 77s 2ms/sample - loss: 5.2541 - accuracy: 0.0959 - val_loss: 5.2577 - val_accuracy: 0.0957\n",
      "Epoch 5/100\n",
      "44687/44687 [==============================] - 69s 2ms/sample - loss: 4.9710 - accuracy: 0.1107 - val_loss: 5.0729 - val_accuracy: 0.1035\n",
      "Epoch 6/100\n",
      "44687/44687 [==============================] - 73s 2ms/sample - loss: 4.7080 - accuracy: 0.1279 - val_loss: 4.8859 - val_accuracy: 0.1143\n",
      "Epoch 7/100\n",
      "44687/44687 [==============================] - 80s 2ms/sample - loss: 4.4437 - accuracy: 0.1514 - val_loss: 4.7067 - val_accuracy: 0.1312\n",
      "Epoch 8/100\n",
      "44687/44687 [==============================] - 86s 2ms/sample - loss: 4.1751 - accuracy: 0.1825 - val_loss: 4.5428 - val_accuracy: 0.1510\n",
      "Epoch 9/100\n",
      "44687/44687 [==============================] - 66s 1ms/sample - loss: 3.9206 - accuracy: 0.2145 - val_loss: 4.3885 - val_accuracy: 0.1716\n",
      "Epoch 10/100\n",
      "44687/44687 [==============================] - 76s 2ms/sample - loss: 3.6844 - accuracy: 0.2484 - val_loss: 4.2574 - val_accuracy: 0.1829\n",
      "Epoch 11/100\n",
      "44687/44687 [==============================] - 86s 2ms/sample - loss: 3.4657 - accuracy: 0.2812 - val_loss: 4.1401 - val_accuracy: 0.1984\n",
      "Epoch 12/100\n",
      "44687/44687 [==============================] - 84s 2ms/sample - loss: 3.2664 - accuracy: 0.3126 - val_loss: 4.0354 - val_accuracy: 0.2137\n",
      "Epoch 13/100\n",
      "44687/44687 [==============================] - 73s 2ms/sample - loss: 3.0815 - accuracy: 0.3473 - val_loss: 3.9638 - val_accuracy: 0.2248\n",
      "Epoch 14/100\n",
      "44687/44687 [==============================] - 75s 2ms/sample - loss: 2.9146 - accuracy: 0.3758 - val_loss: 3.8887 - val_accuracy: 0.2346\n",
      "Epoch 15/100\n",
      "44687/44687 [==============================] - 73s 2ms/sample - loss: 2.7551 - accuracy: 0.4088 - val_loss: 3.8428 - val_accuracy: 0.2423\n",
      "Epoch 16/100\n",
      "44687/44687 [==============================] - 78s 2ms/sample - loss: 2.6161 - accuracy: 0.4323 - val_loss: 3.7929 - val_accuracy: 0.2516\n",
      "Epoch 17/100\n",
      "44687/44687 [==============================] - 70s 2ms/sample - loss: 2.4847 - accuracy: 0.4568 - val_loss: 3.7544 - val_accuracy: 0.2567\n",
      "Epoch 18/100\n",
      "44687/44687 [==============================] - 78s 2ms/sample - loss: 2.3670 - accuracy: 0.4789 - val_loss: 3.7149 - val_accuracy: 0.2679\n",
      "Epoch 19/100\n",
      "44687/44687 [==============================] - 71s 2ms/sample - loss: 2.2588 - accuracy: 0.5005 - val_loss: 3.6852 - val_accuracy: 0.2749\n",
      "Epoch 20/100\n",
      "44687/44687 [==============================] - 65s 1ms/sample - loss: 2.1544 - accuracy: 0.5188 - val_loss: 3.6761 - val_accuracy: 0.2745\n",
      "Epoch 21/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 2.0649 - accuracy: 0.5349 - val_loss: 3.6605 - val_accuracy: 0.2812\n",
      "Epoch 22/100\n",
      "44687/44687 [==============================] - 64s 1ms/sample - loss: 1.9817 - accuracy: 0.5497 - val_loss: 3.6423 - val_accuracy: 0.2857\n",
      "Epoch 23/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.9027 - accuracy: 0.5616 - val_loss: 3.6423 - val_accuracy: 0.2875\n",
      "Epoch 24/100\n",
      "44687/44687 [==============================] - 61s 1ms/sample - loss: 1.8293 - accuracy: 0.5753 - val_loss: 3.6394 - val_accuracy: 0.2872\n",
      "Epoch 25/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.7629 - accuracy: 0.5845 - val_loss: 3.6380 - val_accuracy: 0.2914\n",
      "Epoch 26/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.7000 - accuracy: 0.5945 - val_loss: 3.6480 - val_accuracy: 0.2896\n",
      "Epoch 27/100\n",
      "44687/44687 [==============================] - 64s 1ms/sample - loss: 1.6453 - accuracy: 0.6053 - val_loss: 3.6490 - val_accuracy: 0.2949\n",
      "Epoch 28/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.5843 - accuracy: 0.6160 - val_loss: 3.6505 - val_accuracy: 0.2969\n",
      "Epoch 29/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.5333 - accuracy: 0.6239 - val_loss: 3.6730 - val_accuracy: 0.2918\n",
      "Epoch 30/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.4819 - accuracy: 0.6336 - val_loss: 3.6847 - val_accuracy: 0.2962\n",
      "Epoch 31/100\n",
      "44687/44687 [==============================] - 65s 1ms/sample - loss: 1.4374 - accuracy: 0.6398 - val_loss: 3.6876 - val_accuracy: 0.2967\n",
      "Epoch 32/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.3925 - accuracy: 0.6485 - val_loss: 3.7094 - val_accuracy: 0.2965\n",
      "Epoch 33/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.3504 - accuracy: 0.6550 - val_loss: 3.7171 - val_accuracy: 0.2922\n",
      "Epoch 34/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.3087 - accuracy: 0.6629 - val_loss: 3.7317 - val_accuracy: 0.2944\n",
      "Epoch 35/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.2675 - accuracy: 0.6702 - val_loss: 3.7553 - val_accuracy: 0.2921\n",
      "Epoch 36/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.2285 - accuracy: 0.6788 - val_loss: 3.7802 - val_accuracy: 0.2922\n",
      "Epoch 37/100\n",
      "44687/44687 [==============================] - 61s 1ms/sample - loss: 1.2019 - accuracy: 0.6808 - val_loss: 3.7986 - val_accuracy: 0.2905\n",
      "Epoch 38/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.1668 - accuracy: 0.6860 - val_loss: 3.7992 - val_accuracy: 0.2893\n",
      "Epoch 39/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 1.1358 - accuracy: 0.6932 - val_loss: 3.8323 - val_accuracy: 0.2887\n",
      "Epoch 40/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.1078 - accuracy: 0.6966 - val_loss: 3.8429 - val_accuracy: 0.2901\n",
      "Epoch 41/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.0756 - accuracy: 0.7026 - val_loss: 3.8635 - val_accuracy: 0.2891\n",
      "Epoch 42/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 1.0487 - accuracy: 0.7070 - val_loss: 3.9007 - val_accuracy: 0.2873\n",
      "Epoch 43/100\n",
      "44687/44687 [==============================] - 61s 1ms/sample - loss: 1.0210 - accuracy: 0.7089 - val_loss: 3.9122 - val_accuracy: 0.2852\n",
      "Epoch 44/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 0.9973 - accuracy: 0.7143 - val_loss: 3.9387 - val_accuracy: 0.2818\n",
      "Epoch 45/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 0.9734 - accuracy: 0.7196 - val_loss: 3.9628 - val_accuracy: 0.2832\n",
      "Epoch 46/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 0.9478 - accuracy: 0.7222 - val_loss: 3.9837 - val_accuracy: 0.2814\n",
      "Epoch 47/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 0.9263 - accuracy: 0.7251 - val_loss: 3.9951 - val_accuracy: 0.2824\n",
      "Epoch 48/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 0.9093 - accuracy: 0.7273 - val_loss: 4.0187 - val_accuracy: 0.2810\n",
      "Epoch 49/100\n",
      "44687/44687 [==============================] - 60s 1ms/sample - loss: 0.8833 - accuracy: 0.7326 - val_loss: 4.0367 - val_accuracy: 0.2799\n",
      "Epoch 50/100\n",
      "44687/44687 [==============================] - 60s 1ms/sample - loss: 0.8664 - accuracy: 0.7350 - val_loss: 4.0702 - val_accuracy: 0.2795\n",
      "Epoch 51/100\n",
      "44687/44687 [==============================] - 61s 1ms/sample - loss: 0.8472 - accuracy: 0.7364 - val_loss: 4.1015 - val_accuracy: 0.2769\n",
      "Epoch 52/100\n",
      "44687/44687 [==============================] - 63s 1ms/sample - loss: 0.8328 - accuracy: 0.7378 - val_loss: 4.1020 - val_accuracy: 0.2742\n",
      "Epoch 53/100\n",
      "44687/44687 [==============================] - 62s 1ms/sample - loss: 0.8170 - accuracy: 0.7400 - val_loss: 4.1302 - val_accuracy: 0.2759\n",
      "Epoch 54/100\n",
      "44687/44687 [==============================] - 60s 1ms/sample - loss: 0.8003 - accuracy: 0.7425 - val_loss: 4.1630 - val_accuracy: 0.2750\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44687/44687 [==============================] - 75s 2ms/sample - loss: 0.7857 - accuracy: 0.7441 - val_loss: 4.1868 - val_accuracy: 0.2716\n",
      "Epoch 56/100\n",
      "15872/44687 [=========>....................] - ETA: 44s - loss: 0.7298 - accuracy: 0.7698"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-44d80c69cea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_labs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           batch_size=512)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3217\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 415\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_gru.fit(x_train,\n",
    "          y_train_labs,\n",
    "          epochs=100,\n",
    "          validation_data=(x_test, y_test_labs),\n",
    "          batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aby uzyc fasttext musimy miec plik postaci \n",
    "\n",
    "```_ _ label _ _ <X>  <Text>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with open('train.txt', 'w') as f:\n",
    "    for i, a in enumerate(X_train):\n",
    "        f.write(\"{}\\n\".format(\"__label__\" + str(y_train_labs[i]) + \" \"+a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as f:\n",
    "    for i, a in enumerate(X_test):\n",
    "        f.write(\"{}\\n\".format(\"__label__\" + str(y_test_labs[i]) + \" \"+a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised('train.txt', 'text_classify', epoch=10, dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "result = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [int(r[0]) for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29987916219119226"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_labs, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Podsumowanie\n",
    "Wyniki:\n",
    "\n",
    "Baseline: 0.1871643394199785\n",
    "\n",
    "Google baseline: 0.2136\n",
    "\n",
    "GRU: 0.2716\n",
    "\n",
    "FastText: 0.29987916219119226\n",
    "\n",
    "# Do zrobienia\n",
    "- sieci z attention\n",
    "- sieci syjamskie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bibliografia\n",
    "\n",
    "1. Separable Convolution\n",
    "    - [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "    - [A Basic Introduction to Separable Convolutions](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728)\n",
    "    - [Network Decoupling: From Regular to Depthwise Separable Convolutions](https://arxiv.org/pdf/1808.05517.pdf)\n",
    "    - [Depthwise Separable Convolutions for Neural Machine Translation](https://arxiv.org/pdf/1706.03059.pdf)\n",
    "    - [Depthwise separable convolutions for machine learning](https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/)\n",
    "2. Job title classification\n",
    "    - [Semantic Similarity Strategies for Job Title Classification](https://arxiv.org/pdf/1609.06268v1.pdf)\n",
    "    - [Learning Text Similarity with Siamese Recurrent Networks](http://www.aclweb.org/anthology/W/W16/W16-1617.pdf)\n",
    "3. Text classification\n",
    "    - [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)\n",
    "    - [Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?](https://arxiv.org/pdf/1708.02657.pdf)\n",
    "    - [Fully Convolutional Networks for Text Classification](https://export.arxiv.org/pdf/1902.05575)\n",
    "4. Text classification with siamese\n",
    "    -[Learning Text Similarity with Siamese Recurrent Networks](http://www.aclweb.org/anthology/W/W16/W16-1617.pdf)\n",
    "    \n",
    "    \n",
    "[![Depthwise CNN](http://img.youtube.com/vi/T7o3xvJLuHk/0.jpg)](http://www.youtube.com/watch?v=T7o3xvJLuHk \"Video Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
